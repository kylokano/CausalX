{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the openai batch api to generate reasons and save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up your API key and common headers\n",
    "API_KEY = \"\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"OpenAI Python\"\n",
    "}\n",
    "\n",
    "def upload_batch_file(file_path):\n",
    "    \"\"\"\n",
    "    Uploads the input file for the batch processing job.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    files = {\n",
    "        'purpose': (None, 'batch'),\n",
    "        'file': (file_path, open(file_path, 'rb'))\n",
    "    }\n",
    "    response = requests.post(url, headers={\"Authorization\": f\"Bearer {API_KEY}\"}, files=files)\n",
    "    response_data = response.json()\n",
    "    try:\n",
    "        file_id = response_data['id']\n",
    "        return file_id\n",
    "    except KeyError:\n",
    "        print(\"Error uploading file:\", response_data)\n",
    "        return None\n",
    "\n",
    "def create_batch(input_file_id, endpoint, completion_window=\"24h\", metadata=None):\n",
    "    \"\"\"\n",
    "    Creates a batch processing job using the uploaded input file.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"completion_window\": completion_window\n",
    "    }\n",
    "    if metadata:\n",
    "        data[\"metadata\"] = metadata\n",
    "    response = requests.post(url, headers=HEADERS, data=json.dumps(data))\n",
    "    response_data = response.json()\n",
    "    try:\n",
    "        batch_id = response_data['id']\n",
    "        return batch_id\n",
    "    except KeyError:\n",
    "        print(\"Error creating batch:\", response_data)\n",
    "        return None\n",
    "\n",
    "def check_batch_status(batch_id):\n",
    "    \"\"\"\n",
    "    Checks the status of the batch processing job.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response_data = response.json()\n",
    "    return response_data\n",
    "\n",
    "def retrieve_batch_results(output_file_id, output_file_path):\n",
    "    \"\"\"\n",
    "    Retrieves the results of the completed batch and saves them to a file.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.openai.com/v1/files/{output_file_id}/content\"\n",
    "    response = requests.get(url, headers=HEADERS, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Results saved to {output_file_path}\")\n",
    "    else:\n",
    "        print(\"Error retrieving results:\", response.json())\n",
    "\n",
    "def cancel_batch(batch_id):\n",
    "    \"\"\"\n",
    "    Cancels an ongoing batch processing job.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}/cancel\"\n",
    "    response = requests.post(url, headers=HEADERS)\n",
    "    response_data = response.json()\n",
    "    return response_data\n",
    "\n",
    "def list_batches(limit=10):\n",
    "    \"\"\"\n",
    "    Retrieves a list of all batch processing jobs.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.openai.com/v1/batches?limit={limit}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response_data = response.json()\n",
    "    return response_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 4 batchapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = \"Amazon_Books_small\"\n",
    "output_format_prompt = \"Please give the reasons in the format of list without explaining and prevent any possible data leakage, for example do not show the movie name.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2 jsonl files to batch api format\n",
    "import copy\n",
    "import json\n",
    "template = {\"custom_id\": \"request-1\", \n",
    "            \"method\": \"POST\", \n",
    "            \"url\": \"/v1/chat/completions\", \n",
    "            \"body\": {\"model\": \"gpt-3.5-turbo\", \n",
    "                     \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\n",
    "                     \"max_tokens\":500,\n",
    "                     \"n\":1,\n",
    "                     \"temperature\":0,}}\n",
    "\n",
    "idx = 0\n",
    "output_file_name = f\"prompts_for_generating_explanations/get_reasons_for_LLMAPI_{datasets_name}_batchapi.jsonl\"\n",
    "with open(output_file_name, 'w', encoding='utf8') as f_write:\n",
    "    with open(f\"prompts_for_generating_explanations/get_reasons_for_LLMAPI_{datasets_name}.json\", 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "        tmp_template = copy.deepcopy(template)\n",
    "        for user_id, user_item_data in data.items():\n",
    "            for item_id, prompt in user_item_data['prompt'].items():\n",
    "                if len(prompt) > 1:\n",
    "                    print(prompt)\n",
    "                tmp_template['custom_id'] = f\"{user_id}-{item_id}\"\n",
    "                tmp_template['body']['messages'] = [{\"role\":\"user\",\"content\":prompt[0]+\" \"+output_format_prompt}]\n",
    "                f_write.write(json.dumps(tmp_template, ensure_ascii=False)+'\\n')\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of tokens is too large for batchapi, split into 5000 lines\n",
    "with open(output_file_name, 'r', encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(0, len(lines), 5000):\n",
    "        new_file_path = output_file_name.split(\"/\")[0]+\"/split/\"+output_file_name.split(\"/\")[-1]\n",
    "        with open(new_file_path.split(\".\")[0]+f\"_{i//5000}.jsonl\", 'w', encoding='utf8') as f_write:\n",
    "            f_write.write(''.join(lines[i:i+5000]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: prompts_for_generating_explanations/split/get_reasons_for_LLMAPI_Amazon_Books_small_batchapi_3.jsonl\n",
      "Output file: prompts_for_generating_explanations/split/get_reasons_for_LLMAPI_Amazon_Books_small_batchapi_3_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "processed_file_name = new_file_path.split(\".\")[0]+\"_3.jsonl\"\n",
    "output_file_name = processed_file_name.split(\".\")[0]+\"_output.jsonl\"\n",
    "print(f\"Processing file: {processed_file_name}\")\n",
    "print(f\"Output file: {output_file_name}\")\n",
    "if os.path.exists(output_file_name):\n",
    "    print(f\"Output file already exists: {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from openai_batch_api import upload_batch_file, list_batches, create_batch, check_batch_status, retrieve_batch_results, cancel_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file name prompts_for_generating_explanations/split/get_reasons_for_LLMAPI_Amazon_Books_small_batchapi_3.jsonl\n",
      "Uploaded file ID: file-L8ETpy8PN23Xi9t56xtGbVGt\n"
     ]
    }
   ],
   "source": [
    "# upload file\n",
    "file_id = upload_batch_file(processed_file_name)\n",
    "print(f\"Uploaded file name {processed_file_name}\")\n",
    "print(f\"Uploaded file ID: {file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created batch ID: batch_672cbf4dabd0819096148ec3e42ef063\n"
     ]
    }
   ],
   "source": [
    "# execute task\n",
    "batch_id = create_batch(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "print(f\"Created batch ID: {batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"batch_672cbf4dabd0819096148ec3e42ef063\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-L8ETpy8PN23Xi9t56xtGbVGt\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"completed\",\n",
      "  \"output_file_id\": \"file-eSLzsQ2Qw2znDXuzUUtVGkXB\",\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1730985805,\n",
      "  \"in_progress_at\": 1730985807,\n",
      "  \"expires_at\": 1731072205,\n",
      "  \"finalizing_at\": 1730987008,\n",
      "  \"completed_at\": 1730987411,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 4844,\n",
      "    \"completed\": 4844,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# check task status\n",
    "batch_status = check_batch_status(batch_id)\n",
    "print(json.dumps(batch_status, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to prompts_for_generating_explanations/split/get_reasons_for_LLMAPI_Amazon_Books_small_batchapi_3_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Wait until the batch is completed before retrieving results\n",
    "batch_status = check_batch_status(batch_id)\n",
    "if batch_status.get('status') == 'completed':\n",
    "    output_file_id = batch_status.get('output_file_id')\n",
    "    retrieve_batch_results(output_file_id, output_file_name)\n",
    "else:\n",
    "    print(batch_status.get('status'))\n",
    "    print(\"Batch is not yet completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"batch_672c9106637081909b03021ac330604d\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-iQKoE2DYoYE6uiVzDzeTkZ1z\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"cancelling\",\n",
      "  \"output_file_id\": null,\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1730973958,\n",
      "  \"in_progress_at\": 1730973959,\n",
      "  \"expires_at\": 1731060358,\n",
      "  \"finalizing_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": 1730974479,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 5000,\n",
      "    \"completed\": 4240,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# cancel batch\n",
    "cancel_response = cancel_batch(batch_id)\n",
    "print(json.dumps(cancel_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"batch_672c813c746881908f033c65391ae742\",\n",
      "      \"object\": \"batch\",\n",
      "      \"endpoint\": \"/v1/chat/completions\",\n",
      "      \"errors\": {\n",
      "        \"object\": \"list\",\n",
      "        \"data\": [\n",
      "          {\n",
      "            \"code\": \"token_limit_exceeded\",\n",
      "            \"message\": \"Enqueued token limit reached for gpt-4o-mini-2024-07-18 in organization org-onfIZYZQRYpQGMvatBPCy5kn. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.\",\n",
      "            \"param\": null,\n",
      "            \"line\": null\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"input_file_id\": \"file-Dzt1ddESddlwSpETeC7hlk61\",\n",
      "      \"completion_window\": \"24h\",\n",
      "      \"status\": \"failed\",\n",
      "      \"output_file_id\": null,\n",
      "      \"error_file_id\": null,\n",
      "      \"created_at\": 1730969916,\n",
      "      \"in_progress_at\": null,\n",
      "      \"expires_at\": 1731056316,\n",
      "      \"finalizing_at\": null,\n",
      "      \"completed_at\": null,\n",
      "      \"failed_at\": 1730969921,\n",
      "      \"expired_at\": null,\n",
      "      \"cancelling_at\": null,\n",
      "      \"cancelled_at\": null,\n",
      "      \"request_counts\": {\n",
      "        \"total\": 0,\n",
      "        \"completed\": 0,\n",
      "        \"failed\": 0\n",
      "      },\n",
      "      \"metadata\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"batch_672c80eb44448190b3746efd31efe716\",\n",
      "      \"object\": \"batch\",\n",
      "      \"endpoint\": \"/v1/chat/completions\",\n",
      "      \"errors\": {\n",
      "        \"object\": \"list\",\n",
      "        \"data\": [\n",
      "          {\n",
      "            \"code\": \"token_limit_exceeded\",\n",
      "            \"message\": \"Enqueued token limit reached for gpt-3.5-turbo in organization org-onfIZYZQRYpQGMvatBPCy5kn. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.\",\n",
      "            \"param\": null,\n",
      "            \"line\": null\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"input_file_id\": \"file-tnuPyaioZjJFEQd0ZpoKWzd4\",\n",
      "      \"completion_window\": \"24h\",\n",
      "      \"status\": \"failed\",\n",
      "      \"output_file_id\": null,\n",
      "      \"error_file_id\": null,\n",
      "      \"created_at\": 1730969835,\n",
      "      \"in_progress_at\": null,\n",
      "      \"expires_at\": 1731056235,\n",
      "      \"finalizing_at\": null,\n",
      "      \"completed_at\": null,\n",
      "      \"failed_at\": 1730969839,\n",
      "      \"expired_at\": null,\n",
      "      \"cancelling_at\": null,\n",
      "      \"cancelled_at\": null,\n",
      "      \"request_counts\": {\n",
      "        \"total\": 0,\n",
      "        \"completed\": 0,\n",
      "        \"failed\": 0\n",
      "      },\n",
      "      \"metadata\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"batch_671c95efde0c8190a47ed652665fe891\",\n",
      "      \"object\": \"batch\",\n",
      "      \"endpoint\": \"/v1/chat/completions\",\n",
      "      \"errors\": null,\n",
      "      \"input_file_id\": \"file-FUpB6x6tkiFNM3F5WPDpxLth\",\n",
      "      \"completion_window\": \"24h\",\n",
      "      \"status\": \"completed\",\n",
      "      \"output_file_id\": \"file-MduX3OmFccDNZWfik6geZJ8X\",\n",
      "      \"error_file_id\": null,\n",
      "      \"created_at\": 1729926640,\n",
      "      \"in_progress_at\": 1729926642,\n",
      "      \"expires_at\": 1730013040,\n",
      "      \"finalizing_at\": 1729929594,\n",
      "      \"completed_at\": 1729932420,\n",
      "      \"failed_at\": null,\n",
      "      \"expired_at\": null,\n",
      "      \"cancelling_at\": null,\n",
      "      \"cancelled_at\": null,\n",
      "      \"request_counts\": {\n",
      "        \"total\": 13441,\n",
      "        \"completed\": 13441,\n",
      "        \"failed\": 0\n",
      "      },\n",
      "      \"metadata\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"batch_671c959035f08190984e816b3fb87e6e\",\n",
      "      \"object\": \"batch\",\n",
      "      \"endpoint\": \"/v1/chat/completions\",\n",
      "      \"errors\": {\n",
      "        \"object\": \"list\",\n",
      "        \"data\": [\n",
      "          {\n",
      "            \"code\": \"token_limit_exceeded\",\n",
      "            \"message\": \"Enqueued token limit reached for gpt-4o-2024-08-06 in organization org-onfIZYZQRYpQGMvatBPCy5kn. Limit: 90,000 enqueued tokens. Please try again once some in_progress batches have been completed.\",\n",
      "            \"param\": null,\n",
      "            \"line\": null\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"input_file_id\": \"file-lnyM8j3bUlG6FM2WVKmH0E1e\",\n",
      "      \"completion_window\": \"24h\",\n",
      "      \"status\": \"failed\",\n",
      "      \"output_file_id\": null,\n",
      "      \"error_file_id\": null,\n",
      "      \"created_at\": 1729926544,\n",
      "      \"in_progress_at\": null,\n",
      "      \"expires_at\": 1730012944,\n",
      "      \"finalizing_at\": null,\n",
      "      \"completed_at\": null,\n",
      "      \"failed_at\": 1729926546,\n",
      "      \"expired_at\": null,\n",
      "      \"cancelling_at\": null,\n",
      "      \"cancelled_at\": null,\n",
      "      \"request_counts\": {\n",
      "        \"total\": 0,\n",
      "        \"completed\": 0,\n",
      "        \"failed\": 0\n",
      "      },\n",
      "      \"metadata\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"batch_671c93913b148190a67d30d503b48339\",\n",
      "      \"object\": \"batch\",\n",
      "      \"endpoint\": \"/v1/chat/completions\",\n",
      "      \"errors\": {\n",
      "        \"object\": \"list\",\n",
      "        \"data\": [\n",
      "          {\n",
      "            \"code\": \"token_limit_exceeded\",\n",
      "            \"message\": \"Enqueued token limit reached for gpt-4o-2024-08-06 in organization org-onfIZYZQRYpQGMvatBPCy5kn. Limit: 90,000 enqueued tokens. Please try again once some in_progress batches have been completed.\",\n",
      "            \"param\": null,\n",
      "            \"line\": null\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"input_file_id\": \"file-CZMrakAUKy74WUHbkgGrsIGi\",\n",
      "      \"completion_window\": \"24h\",\n",
      "      \"status\": \"failed\",\n",
      "      \"output_file_id\": null,\n",
      "      \"error_file_id\": null,\n",
      "      \"created_at\": 1729926033,\n",
      "      \"in_progress_at\": null,\n",
      "      \"expires_at\": 1730012433,\n",
      "      \"finalizing_at\": null,\n",
      "      \"completed_at\": null,\n",
      "      \"failed_at\": 1729926036,\n",
      "      \"expired_at\": null,\n",
      "      \"cancelling_at\": null,\n",
      "      \"cancelled_at\": null,\n",
      "      \"request_counts\": {\n",
      "        \"total\": 0,\n",
      "        \"completed\": 0,\n",
      "        \"failed\": 0\n",
      "      },\n",
      "      \"metadata\": null\n",
      "    }\n",
      "  ],\n",
      "  \"first_id\": \"batch_672c813c746881908f033c65391ae742\",\n",
      "  \"last_id\": \"batch_671c93913b148190a67d30d503b48339\",\n",
      "  \"has_more\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# show all tasks\n",
    "batches = list_batches(limit=10)\n",
    "print(json.dumps(batches, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 2 same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19844\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def get_user_item_reasons_pair(raw_batch_data: dict):\n",
    "    union_ids = raw_batch_data[\"custom_id\"]\n",
    "    reasons = raw_batch_data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    return union_ids, reasons\n",
    "\n",
    "out_writter = open(\"openai_reasons_cache/\" + \"openai_reasons_amazonbooks.txt\", 'w', encoding='utf8')\n",
    "idx = 0\n",
    "for i in range(4):\n",
    "    raw_batch_file_path = f\"prompts_for_generating_explanations/split/get_reasons_for_LLMAPI_Amazon_Books_small_batchapi_{i}_output.jsonl\"\n",
    "    with open(raw_batch_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f: \n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            raw_batch_data = json.loads(line)\n",
    "            union_ids, reasons = get_user_item_reasons_pair(raw_batch_data)\n",
    "            out_writter.write(json.dumps({\"union_id\":union_ids, \"reasons\":reasons}, ensure_ascii=False)+\"\\n\")\n",
    "            idx += 1\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
